{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrelisResearch/trelis-colab-chat/blob/main/Trelis_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *About Trelis Chat*\n",
        "\n",
        "A Chat Assistant built on Llama 2.\n",
        "- Upload pdf or text files for analysis.\n",
        "- No data goes to OpenAI.\n",
        "- No data is used for training language models.\n",
        "\n",
        "Find Trelis on [HuggingFace](https://huggingface.co/Trelis)."
      ],
      "metadata": {
        "id": "dDdy-k2BVDAo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGa3YSCgl4C"
      },
      "source": [
        "# Setup and Installation\n",
        "\n",
        "- You can run Trelis Chat on a free Google Colab Notebook.\n",
        "- Save a copy of this notebook: Go to File -> Save a copy in Drive. (optional, but needed if you want to make changes).\n",
        "- Go to the menu -> Runtime -> Change Runtime Type - Select GPU (T4).\n",
        "- Then go to Runtime -> Run all.\n",
        "- It takes about 5-7 mins for the installation (which all happens in the cloud in this notebook).\n",
        "- Once all cells have run, you'll find the chat interface at the bottom.\n",
        "\n",
        "Trelis has no access to your data when you run this notebook. All of your data remains within your Google Drive and Google's computers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the runtime to cpu or gpu. Leave as gpu for Google Colab.\n",
        "runtime = \"gpu\"  # OR \"cpu\"\n",
        "\n",
        "if runtime == \"cpu\":\n",
        "    runtimeFlag = \"cpu\"\n",
        "elif runtime == \"gpu\":\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "else:\n",
        "    print(\"Invalid runtime. Please set it to either 'cpu' or 'gpu'.\")\n",
        "    runtimeFlag = None\n",
        "\n",
        "cache_dir = None # by default, don't set a cache directory\n",
        "print(\"Runtime flag is:\", runtimeFlag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RplPiN-K0-Gb",
        "outputId": "27eb277e-4d8d-4bf5-974c-1e473a755e2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime flag is: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Select the language model and model_basename\n",
        "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\""
      ],
      "metadata": {
        "id": "NCHZB6G2PIDM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Google Drive Mounting (optional)\n",
        "- Allows you to download the model to Google Drive for faster startup next time."
      ],
      "metadata": {
        "id": "X-PAv_Oy1Pr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "oAzlPhWXVKXo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Allow the model to be saved to Google Drive for faster startup next time\n",
        "\n",
        "# # This is the path to the Google Drive folder.\n",
        "# drive_path = \"/content/drive\"\n",
        "\n",
        "# # This is the path where you want to store your cache.\n",
        "# cache_dir_path = os.path.join(drive_path, \"My Drive/huggingface_cache\")\n",
        "\n",
        "# # Check if the Google Drive folder exists. If it does, use it as the cache_dir.\n",
        "# # If not, set cache_dir to None to use the default Hugging Face cache location.\n",
        "# if os.path.exists(drive_path):\n",
        "#     cache_dir = cache_dir_path\n",
        "#     os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists\n",
        "# else:\n",
        "#     cache_dir = None\n",
        "\n",
        "# print(cache_dir)"
      ],
      "metadata": {
        "id": "MH8ZqAEcVLTH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install"
      ],
      "metadata": {
        "id": "qWc_szlJP-Gx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGeTgx1Hgljb",
        "outputId": "e80e29f9-bf64-4b4e-a0b3-0071043c57e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "## some of these are not needed, like bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U auto-gptq\n",
        "!pip install -q -U pdfminer.six"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1zodOgcgqnO"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_r4SL_KmHGs"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model\n",
        "If you have connected to Google Drive, the model will load from there (unless this is your first time connecting, in which case the model will be saved to Drive).\n",
        "- Takes about 6 mins first time around.\n",
        "- Takes about 30s 2nd time onwards with Google Drive."
      ],
      "metadata": {
        "id": "alLhGWu9Ngvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "extrapolation_factor = 1.0 # allows for a max sequence length of 8192 tokens (~6k words) with a factor of 2.0! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM.\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None,\n",
        "        rope_scaling = {\"type\": \"dynamic\", \"factor\": extrapolation_factor}, # allows for a max sequence length of 8192 tokens with a factor of 2.0!!!\n",
        "        cache_dir=cache_dir)\n",
        "\n",
        "# \"\"\"\n",
        "# To download from a specific branch, use the revision parameter, as in this example:\n",
        "\n",
        "# model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "#         revision=\"gptq-4bit-32g-actorder_True\",\n",
        "#         model_basename=model_basename,\n",
        "#         use_safetensors=True,\n",
        "#         trust_remote_code=True,\n",
        "#         device=\"cuda:0\",\n",
        "#         quantize_config=None)\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "KMXDTtTfye8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.max_length*extrapolation_factor)"
      ],
      "metadata": {
        "id": "VPYJ5vUNftKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "vXgoG2ZvuHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Tokenizer"
      ],
      "metadata": {
        "id": "N_bkc-1aQUB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir, use_fast=True) # will use the Rust fast tokenizer if available"
      ],
      "metadata": {
        "id": "kByEN730YOh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BOS token:\", tokenizer.bos_token)\n",
        "print(\"EOS token:\", tokenizer.eos_token)"
      ],
      "metadata": {
        "id": "4CbR5_7t7HWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "import textwrap, json\n",
        "import ipywidgets as widgets\n",
        "import re, time\n",
        "from google.colab import files\n",
        "from pdfminer.high_level import extract_text\n",
        "import io"
      ],
      "metadata": {
        "id": "LLQlqxAIgFzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant that provides accurate and concise responses. Respond in markdown.'\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "# max_doc_length = 50\n",
        "max_doc_length = int(0.75 * model.config.max_length*extrapolation_factor)  # max doc length is 75% of the context length\n",
        "max_doc_words = int(0.75*max_doc_length)"
      ],
      "metadata": {
        "id": "S2YZZel5hjLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(dialogs, temperature=0.3, top_p=0.9, logprobs=False):\n",
        "    torch.cuda.empty_cache()\n",
        "    max_prompt_len = int(0.85 * model.config.max_length*extrapolation_factor)\n",
        "    max_gen_len = int(0.10 * max_prompt_len)\n",
        "\n",
        "    prompt_tokens = []\n",
        "    for dialog in dialogs:\n",
        "        if dialog[0][\"role\"] != \"system\":\n",
        "            dialog = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": DEFAULT_SYSTEM_PROMPT,\n",
        "                }\n",
        "            ] + dialog\n",
        "        dialog_tokens = [tokenizer(\n",
        "            f\"{B_INST} {B_SYS}{(dialog[0]['content']).strip()}{E_SYS}{(dialog[1]['content']).strip()} {E_INST}\",\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        ).input_ids.to(runtimeFlag)]\n",
        "        for i in range(2, len(dialog), 2):\n",
        "            user_tokens = tokenizer(\n",
        "                f\"{B_INST} {(dialog[i+1]['content']).strip()} {E_INST}\",\n",
        "                return_tensors=\"pt\",\n",
        "                add_special_tokens=True\n",
        "            ).input_ids.to(runtimeFlag)\n",
        "            assistant_w_eos = dialog[i]['content'].strip() + tokenizer.eos_token\n",
        "            assistant_tokens = tokenizer(\n",
        "                            assistant_w_eos,\n",
        "                            return_tensors=\"pt\",\n",
        "                            add_special_tokens=False\n",
        "                        ).input_ids.to(runtimeFlag)\n",
        "            tokens = torch.cat([assistant_tokens, user_tokens], dim=-1)\n",
        "            dialog_tokens.append(tokens)\n",
        "        prompt_tokens.append(torch.cat(dialog_tokens, dim=-1))\n",
        "\n",
        "    input_ids = prompt_tokens[0]\n",
        "    if len(input_ids[0]) > max_prompt_len:\n",
        "        return \"\\n\\n **The language model's input limit has been reached. Clear the chat and start afresh!**\"\n",
        "\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=max_gen_len,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    new_tokens = generation_output[0][input_ids.shape[-1]:]\n",
        "    # print(tokenizer.decode(input_ids[0], skip_special_tokens=False))\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "cXBr4M25gbet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def print_wrapped(text, width=80):\n",
        "#     # Regular expression pattern to detect code blocks\n",
        "#     code_pattern = r'```(.+?)```'\n",
        "#     matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
        "#     if not matches:\n",
        "#         display(Markdown('\\n'.join(textwrap.wrap(textwrap.dedent(text), width=width))))\n",
        "#         return\n",
        "\n",
        "#     start = 0\n",
        "#     for match in matches:\n",
        "#         before_code = text[start:match.start()].strip()\n",
        "#         if before_code:\n",
        "#             display(Markdown('\\n'.join(textwrap.wrap(textwrap.dedent(before_code), width=width))))\n",
        "\n",
        "#         code = match.group(0).strip()  # Extract code block\n",
        "#         display(Markdown('\\n' + code + '\\n'))  # Display code block with space before and after\n",
        "\n",
        "#         start = match.end()\n",
        "\n",
        "#     after_code = text[start:].strip()  # Text after the last code block\n",
        "#     if after_code:\n",
        "#         display(Markdown('\\n'.join(textwrap.wrap(textwrap.dedent(after_code), width=width))))\n",
        "\n",
        "def print_wrapped(text):\n",
        "    # Regular expression pattern to detect code blocks\n",
        "    code_pattern = r'```(.+?)```'\n",
        "    matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
        "\n",
        "    if not matches:\n",
        "        # If there are no code blocks, display the entire text as Markdown\n",
        "        display(Markdown(text))\n",
        "        return\n",
        "\n",
        "    start = 0\n",
        "    for match in matches:\n",
        "        # Display the text before the code block as Markdown\n",
        "        before_code = text[start:match.start()].strip()\n",
        "        if before_code:\n",
        "            display(Markdown(before_code))\n",
        "\n",
        "        # Display the code block\n",
        "        code = match.group(0).strip()  # Extract code block\n",
        "        display(Markdown(code))  # Display code block\n",
        "\n",
        "        start = match.end()\n",
        "\n",
        "    # Display the text after the last code block as Markdown\n",
        "    after_code = text[start:].strip()  # Text after the last code block\n",
        "    if after_code:\n",
        "        display(Markdown(after_code))\n",
        "\n",
        "\n",
        "def grab_and_shorten_text(max_doc_length):\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    # Check the file extension\n",
        "    if file_name.endswith('.txt'):\n",
        "        text = uploaded[file_name].decode()\n",
        "    elif file_name.endswith('.pdf'):\n",
        "        pdf_bytes = io.BytesIO(uploaded[file_name])\n",
        "        text = extract_text(pdf_bytes)\n",
        "    else:\n",
        "        raise ValueError('Unsupported file type. Please upload a .txt or .pdf file.')\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        print(\"Shortening the text...\")\n",
        "\n",
        "    tokens = tokenizer.encode(text, truncation=True, max_length=max_doc_length, return_tensors='pt')\n",
        "\n",
        "    shortened_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return file_name, shortened_text\n",
        "\n",
        "dialog_history = [{\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT}]\n",
        "\n",
        "button = widgets.Button(description=\"Send\")\n",
        "upload_button = widgets.Button(description=\"Upload .txt or .pdf\")\n",
        "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
        "\n",
        "output_log = widgets.Output()\n",
        "\n",
        "# Define the 'Send' button click event handler\n",
        "def on_button_clicked(b):\n",
        "    user_input = text.value\n",
        "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    text.value = ''\n",
        "\n",
        "    button.description = 'Processing...'\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "    assistant_response = generate_response([dialog_history])\n",
        "\n",
        "    button.description = 'Send'\n",
        "\n",
        "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Create an output widget for alerts\n",
        "alert_out = widgets.Output()\n",
        "\n",
        "# Define the 'Upload' button click event handler\n",
        "def on_upload_button_clicked(b):\n",
        "\n",
        "    file_name, uploaded_text = grab_and_shorten_text(max_doc_length)\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        print(f\"Upload successful: {file_name}, processing the file...\")\n",
        "\n",
        "    user_input = f\"Uploaded document [{file_name}]: {uploaded_text}\"\n",
        "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    time.sleep(0.1)  # slight delay to ensure order\n",
        "\n",
        "    assistant_input = f\"You have uploaded text from {file_name}\"\n",
        "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_input})\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "    with alert_out:\n",
        "        clear_output()  # Clear the previous alert\n",
        "        # print(f\"File processing completed.\")\n",
        "\n",
        "upload_button.on_click(on_upload_button_clicked)\n",
        "\n",
        "clear_button = widgets.Button(description=\"Clear Chat\")\n",
        "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
        "\n",
        "def on_clear_button_clicked(b):\n",
        "    # Clear the dialog history\n",
        "    dialog_history.clear()\n",
        "    # Add back the initial system prompt\n",
        "    dialog_history.append({\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT})\n",
        "    # Clear the output log\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "\n",
        "clear_button.on_click(on_clear_button_clicked)\n",
        "\n",
        "def save_chat(b):\n",
        "    # Serialize the chat history into a JSON string\n",
        "    chat_json = json.dumps(dialog_history)\n",
        "\n",
        "    # Write the chat history to a temporary file\n",
        "    with open('chat_history.json', 'w') as f:\n",
        "        f.write(chat_json)\n",
        "\n",
        "    # Download the file\n",
        "    files.download('chat_history.json')\n",
        "\n",
        "save_button = widgets.Button(description=\"Save Chat\")\n",
        "save_button.on_click(save_chat)"
      ],
      "metadata": {
        "id": "k_RJObixH_HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to upload chat\n",
        "def upload_chat(b):\n",
        "    # Upload the file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the file name\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "\n",
        "    # Ensure the file is a .json file\n",
        "    if not file_name.endswith('.json'):\n",
        "        print('Error: Incorrect file type. Please upload a .json file.')\n",
        "        return\n",
        "\n",
        "    # Load the content of the file\n",
        "    chat_data = uploaded[file_name].decode()\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    try:\n",
        "        global dialog_history\n",
        "        dialog_history = json.loads(chat_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print('Error: File is not in the correct format. Please upload a properly formatted .json file.')\n",
        "        return\n",
        "\n",
        "    with output_log:\n",
        "        clear_output()\n",
        "        for message in dialog_history:\n",
        "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
        "\n",
        "# Create the upload button and set the on_click event handler\n",
        "upload_chat_button = widgets.Button(description=\"Upload Chat\")\n",
        "upload_chat_button.on_click(upload_chat)\n"
      ],
      "metadata": {
        "id": "-ZI8fCE7KERH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import HBox, VBox\n",
        "\n",
        "# Create the title with Markdown\n",
        "title = f\"#Trelis Chat\\n (uploaded files will be shortened to {max_doc_words} words)\\n\" + \"\\n\"\n",
        "\n",
        "# Assuming that output_log, alert_out, and text are other widgets or display elements...\n",
        "first_row = HBox([button, clear_button, upload_button])  # Arrange these buttons horizontally\n",
        "second_row = HBox([save_button, upload_chat_button])  # Arrange these buttons horizontally\n",
        "\n",
        "# Arrange the two rows of buttons and other display elements vertically\n",
        "layout = VBox([output_log, alert_out, text, first_row, second_row])"
      ],
      "metadata": {
        "id": "Uy0iMfgLKKk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat"
      ],
      "metadata": {
        "id": "qiAEnqKuPGoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(title))\n",
        "display(layout)"
      ],
      "metadata": {
        "id": "_r5fQloTPGx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}